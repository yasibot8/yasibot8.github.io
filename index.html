)<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Human2Bot</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Human2Bot: Learning Zero-Shot Reward Functions for Robotic Manipulation from Human Demonstrations</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yasir Salam</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yinbei Li</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jonas Herzog</a><sup></sup>,</span>
                      <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jiaqiang Yang<sup>*</sup></a>
                        </span>
                    </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Zhejiang University, China <br>*Corresponding Author</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (Accepted)</span>
                      </a>
                    </span>

                    

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Human2Bot.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       H2B: Zero-shot Human-to-robot Task Transfer in Simulation and Real-World Environments 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Developing effective reward functions is crucial for robot learning, as they guide behavior and facilitate adaptation to human-like tasks. We present Human2Bot (H2B), advancing the learning of such a generalized multi-task reward function that can be used zero-shot to execute unknown tasks in unseen environments. H2B is a newly designed task similarity estimation model that is trained on a large dataset of human videos. The model determines whether two videos from different environments represent the same task. At test time, the model serves as a reward function, evaluating how closely a robot’s execution matches the human demonstration. While previous approaches necessitate robot-specific data to learn reward functions or policies, our method can learn without any robot datasets. To achieve generalization in robotic environments, we incorporate a domain augmentation process that generates synthetic videos with varied visual appearances resembling simulation environments, alongside a multi-scale inter-frame attention mechanism that aligns human and robot task understanding. Finally, H2B is integrated with Visual Model Predictive Control (VMPC) to perform manipulation tasks in simulation and on the xARM6 robot in real-world settings. Our approach outperforms previous methods in simulated and real-world environments trained solely on human data, eliminating the need for privileged robot datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- First image -->
      <h2 class="title is-3 has-text-centered">Introduction</h2>
      <div class="mb-6">
        <img src="static/images/Introduction.png" alt="Human demonstration and robot learning" style="width: 650px; height: auto; display: block; margin: 0 auto;"/>
        <p style="text-align: justify; max-width: 650px; margin: 20px auto; font-size: 16px; line-height: 1.6;">
          <strong>Left:</strong> A human demonstration is given (e.g., closing a drawer). <strong>Right:</strong> The robot learns to deduce the task through its interactions with the environment by executing actions and recording observations. H2B evaluates each sequence of observations based on its similarity to the human demonstration and provides reward on the robot’s performance, guiding it to accomplish the task like the human.
        </p>
      </div>

      <!-- Second image -->
      <h2 class="title is-3 has-text-centered">Proposed Method </h2>
      <div class="mb-6">
        <img src="static/images/Architecture.png" alt="Vision Dynamics Profiler architecture" style="width: 1000px; height: auto; display: block; margin: 0 auto;"/>
        <p style="text-align: justify; max-width: 1000px; margin: 20px auto; font-size: 16px; line-height: 1.6;">
          The <strong>Vision Dynamics Profiler (VDP)</strong> takes conditionally augmented video frames as input to produce frame-level features through a pre-trained encoder. The feature reduction and multi-scale inter-frame attention layers further process these features to output a representation for each video. <strong>Frame-to-frame cosine similarity</strong> is then calculated between the two videos. The <strong>Similarity Fusion Network (SFN)</strong> processes the similarity vector through a series of 1D-convolution layers to generate a similarity score.
        </p>
      </div>
      <!-- Third image -->
      <div class="mb-6">
        <img src="static/images/iCEM.png" alt="Vision Dynamics Profiler architecture" style="width: 900px; height: auto; display: block; margin: 0 auto;"/>
        <p style="text-align: justify; max-width: 900px; margin: 20px auto; font-size: 16px; line-height: 1.6;">
          During the <strong>final task execution</strong>, we used a single human demonstration of the task and sampled robot trajectories from the environment as input. We evaluate all these trajectories using </strong>H2B</strong> to determine their similarity to the demonstration, and assigning rewards accordingly. </strong>iCEM</strong> then optimizes its parameters based on the top-K trajectories by rewards to generate trajectories in the next iteration that closely mimic the demonstrated task.
        </p>
      </div>

       <!-- Fourth image -->
      <h2 class="title is-3 has-text-centered">Experimental Setup and Results </h2>
      <div class="mb-6">
        <img src="static/images/Training.png" alt="Vision Dynamics Profiler architecture" style="width: 900px; height: auto; display: block; margin: 0 auto;"/>
        <p style="text-align: justify; max-width: 900px; margin: 20px auto; font-size: 16px; line-height: 1.6;">
       <strong>Training (left):</strong> The agent learns a reward function from numerous human videos encompassing various tasks and environments. <strong>Inference (right):</strong> Evaluation is conducted across diverse simulated environments and real-robot scenarios, both involving tabletop settings with various interactive objects.
        </p>
      </div>
      
      <!-- Fifth image -->
      <div class="mb-6">
        <img src="static/images/table1.png" alt="Vision Dynamics Profiler architecture" style="width: 900px; height: auto; display: block; margin: 0 auto;"/>
        </p>
      </div>
      
      <!-- Sixth image -->
      <div class="mb-6">
        <img src="static/images/expd.png" alt="Vision Dynamics Profiler architecture" style="width: 900px; height: auto; display: block; margin: 0 auto;"/>
        <p style="text-align: justify; max-width: 900px; margin: 20px auto; font-size: 16px; line-height: 1.6;">
       Success rate for three target tasks, showing the impact of increasing non-target training tasks on generalization. The dotted line represents the average success rate.
        </p>
      </div>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End image display -->

  <!-- Image display side by side using Bulma columns -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <!-- Seventh image -->
        <div class="column is-half">
          <img src="static/images/table2.png" alt="Vision Dynamics Profiler architecture" style="width: 100%; height: auto; display: block; margin: 0 auto;"/>
        </div>

        <!-- Eighth image -->
        <div class="column is-half">
          <img src="static/images/expf.png" alt="Vision Dynamics Profiler architecture" style="width: 100%; height: auto; display: block; margin: 0 auto;"/>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
